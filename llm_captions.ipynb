{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "machine_shape": "hm",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU",
    "gpuClass": "standard"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/baroodya/llm-captions/blob/main/llm_captions.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "IwuqeaepQ8QA",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "a4d9878e-f3e3-41e1-f1f0-60785cad8456"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting transformers\n",
            "  Downloading transformers-4.28.1-py3-none-any.whl (7.0 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m7.0/7.0 MB\u001b[0m \u001b[31m31.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from transformers) (23.1)\n",
            "Collecting huggingface-hub<1.0,>=0.11.0\n",
            "  Downloading huggingface_hub-0.14.1-py3-none-any.whl (224 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m224.5/224.5 kB\u001b[0m \u001b[31m10.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.10/dist-packages (from transformers) (6.0)\n",
            "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.10/dist-packages (from transformers) (4.65.0)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.10/dist-packages (from transformers) (2022.10.31)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from transformers) (3.12.0)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.10/dist-packages (from transformers) (1.22.4)\n",
            "Collecting tokenizers!=0.11.3,<0.14,>=0.11.1\n",
            "  Downloading tokenizers-0.13.3-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (7.8 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m7.8/7.8 MB\u001b[0m \u001b[31m79.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from transformers) (2.27.1)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.10/dist-packages (from huggingface-hub<1.0,>=0.11.0->transformers) (2023.4.0)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub<1.0,>=0.11.0->transformers) (4.5.0)\n",
            "Requirement already satisfied: charset-normalizer~=2.0.0 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (2.0.12)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (3.4)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (2022.12.7)\n",
            "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (1.26.15)\n",
            "Installing collected packages: tokenizers, huggingface-hub, transformers\n",
            "Successfully installed huggingface-hub-0.14.1 tokenizers-0.13.3 transformers-4.28.1\n",
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting pycocoevalcap\n",
            "  Downloading pycocoevalcap-1.2-py3-none-any.whl (104.3 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m104.3/104.3 MB\u001b[0m \u001b[31m6.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: pycocotools>=2.0.2 in /usr/local/lib/python3.10/dist-packages (from pycocoevalcap) (2.0.6)\n",
            "Requirement already satisfied: matplotlib>=2.1.0 in /usr/local/lib/python3.10/dist-packages (from pycocotools>=2.0.2->pycocoevalcap) (3.7.1)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (from pycocotools>=2.0.2->pycocoevalcap) (1.22.4)\n",
            "Requirement already satisfied: contourpy>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib>=2.1.0->pycocotools>=2.0.2->pycocoevalcap) (1.0.7)\n",
            "Requirement already satisfied: kiwisolver>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib>=2.1.0->pycocotools>=2.0.2->pycocoevalcap) (1.4.4)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.10/dist-packages (from matplotlib>=2.1.0->pycocotools>=2.0.2->pycocoevalcap) (0.11.0)\n",
            "Requirement already satisfied: fonttools>=4.22.0 in /usr/local/lib/python3.10/dist-packages (from matplotlib>=2.1.0->pycocotools>=2.0.2->pycocoevalcap) (4.39.3)\n",
            "Requirement already satisfied: pillow>=6.2.0 in /usr/local/lib/python3.10/dist-packages (from matplotlib>=2.1.0->pycocotools>=2.0.2->pycocoevalcap) (8.4.0)\n",
            "Requirement already satisfied: pyparsing>=2.3.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib>=2.1.0->pycocotools>=2.0.2->pycocoevalcap) (3.0.9)\n",
            "Requirement already satisfied: python-dateutil>=2.7 in /usr/local/lib/python3.10/dist-packages (from matplotlib>=2.1.0->pycocotools>=2.0.2->pycocoevalcap) (2.8.2)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from matplotlib>=2.1.0->pycocotools>=2.0.2->pycocoevalcap) (23.1)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.10/dist-packages (from python-dateutil>=2.7->matplotlib>=2.1.0->pycocotools>=2.0.2->pycocoevalcap) (1.16.0)\n",
            "Installing collected packages: pycocoevalcap\n",
            "Successfully installed pycocoevalcap-1.2\n",
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting openai\n",
            "  Downloading openai-0.27.6-py3-none-any.whl (71 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m71.9/71.9 kB\u001b[0m \u001b[31m2.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting aiohttp\n",
            "  Downloading aiohttp-3.8.4-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.0 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.0/1.0 MB\u001b[0m \u001b[31m18.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: requests>=2.20 in /usr/local/lib/python3.10/dist-packages (from openai) (2.27.1)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (from openai) (4.65.0)\n",
            "Requirement already satisfied: charset-normalizer~=2.0.0 in /usr/local/lib/python3.10/dist-packages (from requests>=2.20->openai) (2.0.12)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests>=2.20->openai) (3.4)\n",
            "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests>=2.20->openai) (1.26.15)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests>=2.20->openai) (2022.12.7)\n",
            "Collecting aiosignal>=1.1.2\n",
            "  Downloading aiosignal-1.3.1-py3-none-any.whl (7.6 kB)\n",
            "Collecting multidict<7.0,>=4.5\n",
            "  Downloading multidict-6.0.4-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (114 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m114.5/114.5 kB\u001b[0m \u001b[31m15.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting async-timeout<5.0,>=4.0.0a3\n",
            "  Downloading async_timeout-4.0.2-py3-none-any.whl (5.8 kB)\n",
            "Collecting frozenlist>=1.1.1\n",
            "  Downloading frozenlist-1.3.3-cp310-cp310-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl (149 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m149.6/149.6 kB\u001b[0m \u001b[31m19.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->openai) (23.1.0)\n",
            "Collecting yarl<2.0,>=1.0\n",
            "  Downloading yarl-1.9.2-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (268 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m268.8/268.8 kB\u001b[0m \u001b[31m32.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: multidict, frozenlist, async-timeout, yarl, aiosignal, aiohttp, openai\n",
            "Successfully installed aiohttp-3.8.4 aiosignal-1.3.1 async-timeout-4.0.2 frozenlist-1.3.3 multidict-6.0.4 openai-0.27.6 yarl-1.9.2\n"
          ]
        }
      ],
      "source": [
        "!python -m pip install transformers\n",
        "!python -m pip install pycocoevalcap\n",
        "!python -m pip install openai"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# import some common libraries\n",
        "import numpy as np\n",
        "import os, json\n",
        "import cv2\n",
        "from tqdm import tqdm\n",
        "import random\n",
        "\n",
        "import torch\n",
        "from itertools import islice\n",
        "\n",
        "from transformers import pipeline\n",
        "from transformers import GenerationConfig\n",
        "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
        "\n",
        "from pycocotools.coco import COCO\n",
        "from pycocoevalcap.eval import COCOEvalCap\n",
        "\n",
        "import os\n",
        "import openai"
      ],
      "metadata": {
        "id": "57trFNcqSNQr"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Set Up LLM"
      ],
      "metadata": {
        "id": "HScczgifome4"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "model_name = \"gpt-3\"\n",
        "warmup = True\n",
        "max_new_tokens = 50\n",
        "device = torch.device(\"cuda\") if torch.cuda.is_available() else torch.device(\"cpu\")\n",
        "if model_name == \"gpt-3\":\n",
        "    openai.organization = \"org-INV4GdfBST6aC9DFNHjcLtcT\"\n",
        "    openai.api_key = \"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\"\n",
        "else:\n",
        "    tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
        "    model = AutoModelForCausalLM.from_pretrained(model_name)\n",
        "\n",
        "    print(f\"On device: {device}\")\n",
        "    model.to(device)\n",
        "    print(model.device)\n",
        "\n",
        "    generation_config = GenerationConfig.from_pretrained(model_name)\n",
        "    generation_config.pad_token_id = 50256\n",
        "    generation_config.early_stopping = True\n",
        "    generation_config.num_beams = 3\n",
        "    generation_config.max_new_tokens = max_new_tokens"
      ],
      "metadata": {
        "id": "_V1shWHQoeOl"
      },
      "execution_count": 24,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/gdrive')\n",
        "\n",
        "cat_file = \"/content/gdrive/MyDrive/Semester 8/COS 485/Final Project/detectron2_val2017_categories.json\"\n",
        "outputs_file = \"/content/gdrive/MyDrive/Semester 8/COS 485/Final Project/detectron2_val2017_outputs_captions.json\"\n",
        "\n",
        "with open(cat_file, \"r\") as f:\n",
        "    cat_names = json.load(f)\n",
        "with open(outputs_file, \"r\") as f:\n",
        "    outputs_captions = json.load(f)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "uyOqs680uRb7",
        "outputId": "4c9bcd56-486f-47bc-fa0e-553a511bbd52"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/gdrive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def create_prompt_features(classes, boxes, k=5):\n",
        "    features = \"\"\n",
        "\n",
        "    for i in range(min(k, len(classes))):\n",
        "        label = classes[i]\n",
        "        box = boxes[i]\n",
        "\n",
        "        center_x = abs(box[0])\n",
        "        center_y = abs(box[1])\n",
        "\n",
        "        width = abs(box[2])\n",
        "        height = abs(box[3])\n",
        "        \n",
        "        features += f\"A {width:.0f} by {height:.0f} \\\"{label}\\\" centered at ({center_x:.0f}, {center_y:.0f}).\"\n",
        "        if i < k-1: features += \"\\n\"\n",
        "    return features\n",
        "\n",
        "def create_background_features(stuff_list):\n",
        "    stuff_list = [\"\\\"\" + x + \"\\\"\" for x in stuff_list]\n",
        "    return \"The background contains objects with the following labels: \" + \", \".join(stuff_list)\n",
        "\n",
        "\n",
        "def create_prompt(output_data, k=5):\n",
        "    im_width, im_height, _ = output_data[\"im_dims\"]\n",
        "    \n",
        "    background = \"\"\n",
        "    if \"background_stuff\" in output_data:\n",
        "        background = create_background_features(output_data[\"background_stuff\"])\n",
        "\n",
        "    features = create_prompt_features(output_data[\"detected_cats\"], output_data[\"pred_boxes\"], k)\n",
        "    prompt = f\"Caption a {im_width} pixel by {im_height} pixel image with the following features:\\n{features}\\n{background}. In your caption, make assumptions about what else could be in the image.\\nA short, informative caption for this image is:\"\n",
        "    return prompt\n",
        "\n",
        "def create_warmed_up_prompt(output_data, k=5):\n",
        "    im_width, im_height, _ = output_data[\"im_dims\"]\n",
        "\n",
        "    background = \"\"\n",
        "    if \"background_stuff\" in output_data:\n",
        "        background = create_background_features(output_data[\"background_stuff\"])\n",
        "\n",
        "    features = create_prompt_features(output_data[\"detected_cats\"], output_data[\"pred_boxes\"], k)\n",
        "    prompt = f\"Based on those steps, caption a {im_width} pixel by {im_height} pixel image with the following features:\\n{features}\\nThe background contains objects with the following labels: {background}. In your caption, make assumptions about what else could be in the image. Do no output the steps you followed, only output the caption.\\nA short, conceptual caption for this image is:\"\n",
        "    return prompt\n",
        "\n",
        "def create_few_shot_prompt(output_data, examples, k=5):\n",
        "    im_width, im_height, _ = output_data[\"im_dims\"]\n",
        "    test_features = create_prompt_features(output_data[\"detected_cats\"], output_data[\"pred_boxes\"], k)\n",
        "    example_prompts = \"\"\n",
        "    for example in examples:\n",
        "        example_prompt = create_prompt(example, k=k)\n",
        "        example_prompts += f\"The prompt \\\"{example_prompt}\\\" would yield the caption \\\"{example['captions'][0]}\\\"\"\n",
        "    prompt = f\"Create a simple caption for a {im_width} pixel by {im_height} pixel image with the following features:\\n{test_features}\\n Use the following examples:\\n{example_prompts}\\n A good caption is:\"\n",
        "    return prompt"
      ],
      "metadata": {
        "id": "FKwf144HSbLK"
      },
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Select a random image to use to test the code and warmup the model\n",
        "id, output = random.choice(list(outputs_captions.items()))\n",
        "print(id)\n",
        "print(output[\"captions\"])\n",
        "\n",
        "prompt = create_warmed_up_prompt(output, k=5)\n",
        "answer = \"\"\n",
        "if model_name == \"gpt-3\":\n",
        "    response = openai.ChatCompletion.create(\n",
        "        model=\"gpt-3.5-turbo\",\n",
        "        messages=[{\"role\": \"user\", \"content\": prompt}],\n",
        "        max_tokens=max_new_tokens,\n",
        "        temperature=0.7\n",
        "    )\n",
        "    answer = response['choices'][0]['message']['content']\n",
        "else:\n",
        "    inputs = tokenizer(prompt, return_tensors=\"pt\").to(device)\n",
        "\n",
        "    llm_output = model.generate(**inputs, generation_config=generation_config)\n",
        "\n",
        "    decoded_output = tokenizer.batch_decode(llm_output, skip_special_tokens=True)\n",
        "\n",
        "    answer = decoded_output[0][len(prompt):]\n",
        "    answer = answer.split(\"\\n\")[0]\n",
        "print(f\"\"\"\n",
        "Using {model_name}.\n",
        "Example Prompt: {prompt}\n",
        "\n",
        "Example Caption: {answer}\"\"\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Th-epk6PSIZu",
        "outputId": "28b6f666-e571-4e18-e8f3-752d876498e9"
      },
      "execution_count": 71,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "573626\n",
            "['Wild animals standing in a  forest next to a river.', 'Two horses stand on rocks near a river. ', 'Two dogs walking on rocks next to a creek.', 'There are horses standing in the rocks near the water.', 'Two large animals walking on rocks along the side of river. ']\n",
            "\n",
            "Using gpt-3.\n",
            "Example Prompt: Based on those steps, caption a 375 pixel by 500 pixel image with the following features:\n",
            "A 31 by 59 \"bird\" centered at (300, 175).\n",
            "A 63 by 56 \"bear\" centered at (212, 156).\n",
            "A 64 by 41 \"bird\" centered at (74, 233).\n",
            "\n",
            "The background contains objects with the following labels: The background contains objects with the following labels: \"river\", \"tree\", \"rock\". In your caption, make assumptions about what else could be in the image. Do no output the steps you followed, only output the caption.\n",
            "A short, conceptual caption for this image is:\n",
            "\n",
            "Example Caption: \"Nature's Harmony: A serene river flows amidst a lush green forest, where a majestic bear roams and two birds take flight in the distance, with rocks scattered along the way.\"\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "messages = []\n",
        "if model_name == \"gpt-3\" and warmup:\n",
        "    interaction_guide = \"give a step by step guide on how you, chatGPT, could figure out how two objects in an image could be determined to be interacting. You are given their width and height in pixels, their label, and their centerpoint in pixels.\"\n",
        "    messages.append({\"role\": \"user\", \"content\": interaction_guide})\n",
        "    guide_response = openai.ChatCompletion.create(\n",
        "        model=\"gpt-3.5-turbo\",\n",
        "        messages=messages\n",
        "    )\n",
        "    guide_answer = guide_response['choices'][0]['message']['content']\n",
        "    messages.append({\"role\": \"assistant\", \"content\": guide_answer})\n",
        "\n",
        "    im_width, im_height, _ = output[\"im_dims\"]\n",
        "    example_interactions = f\"Caption a {im_width} pixel by {im_height} pixel image with the following features:\\n{create_prompt_features(output['detected_cats'], output['pred_boxes'])}.\\nGive a step by step guide to how you could determine the relative positions of each object, then give the relative positions of each object to each other object.\"\n",
        "    messages.append({\"role\": \"user\", \"content\": example_interactions})\n",
        "    example_response = openai.ChatCompletion.create(\n",
        "        model=\"gpt-3.5-turbo\",\n",
        "        messages=messages\n",
        "    )\n",
        "    example_answer = example_response['choices'][0]['message']['content']\n",
        "    messages.append({\"role\": \"assistant\", \"content\": example_answer})\n",
        "    print(messages)\n"
      ],
      "metadata": {
        "id": "Z8T9tdjBZczh",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "8f470f74-bef5-46a4-d214-8138e37441aa"
      },
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[{'role': 'user', 'content': 'give a step by step guide on how you, chatGPT, could figure out how two objects in an image could be determined to be interacting. You are given their width and height in pixels, their label, and their centerpoint in pixels.'}, {'role': 'assistant', 'content': 'As an AI language model, I can give you a step-by-step guide on how you can determine if two objects in an image are interacting based on their width and height in pixels, label, and their center points in pixels:\\n\\n1. First, you need to visually inspect the image and identify the two objects you want to check for interaction. You can do this by looking for shapes that are close together, partially overlapping, or seem to be connected in some way.\\n\\n2. Once you have identified the objects, you can use their width and height in pixels to estimate their size and how much they occupy the image area.\\n\\n3. Next, check the center points of the objects in pixels. If the two center points are close together, it can mean that the objects are interacting, such as two people embracing or two balls colliding.\\n\\n4. Look at the label of each object, if it has one, and see if it indicates that the objects are related or connected in some way. For example, if one object is labeled \"car\" and the other is labeled \"road,\" it is likely that the car is driving on the road.\\n\\n5. If the objects have properties such as velocity, momentum, or acceleration, you can use that information to determine if they are interacting. For example, if one object is moving towards the other, it may indicate that they are interacting or about to interact.\\n\\n6. Finally, consider other contextual factors that may indicate interaction, such as the environment or the scene. For example, if two people are standing close together and facing each other, it may indicate a conversation or interaction.\\n\\nOverall, determining if two objects are interacting in an image requires careful analysis of their properties, their relationship to each other, and the contextual factors within the image.'}, {'role': 'user', 'content': 'Caption a 640 pixel by 426 pixel image with the following features:\\nA 174 by 302 \"person\" centered at (221, 216).\\nA 107 by 100 \"tennis racket\" centered at (132, 167).\\nA 16 by 17 \"sports ball\" centered at (315, 165).\\n.\\nGive a step by step guide to how you could determine the relative positions of each object, then give the relative positions of each object to each other object.'}, {'role': 'assistant', 'content': 'Caption: A person holding a tennis racket reaching for a sports ball.\\n\\nStep-by-Step Guide:\\n1. Identify the three labeled objects: person, tennis racket, and sports ball.\\n2. Note their respective sizes in pixels: person (174 by 302), tennis racket (107 by 100), sports ball (16 by 17).\\n3. Locate the centerpoint of each object in pixels: person (221, 216), tennis racket (132, 167), sports ball (315, 165).\\n4. Use the centerpoints to determine the relative positions of each object. For example, the person is to the right and slightly above the tennis racket, and the sports ball is to the left and slightly below the tennis racket.\\n5. Consider the orientation and implied motion of each object to refine the relative positions. For example, the person appears to be reaching for the sports ball with the tennis racket, while the sports ball appears to be suspended in the air.\\n6. Use the relative positions of each object to describe their arrangement. For example, \"The person is holding the tennis racket and reaching towards the sports ball, which is to the left and below the racket.\"'}]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def prompting(imgs, device, num_examples=0, k=5, n=200, warmup=False):\n",
        "    img_list = list(imgs.items())\n",
        "    example_imgs = None\n",
        "    prompt_func = create_prompt\n",
        "    if warmup and model_name == \"gpt-3\":\n",
        "        prompt_func = create_warmed_up_prompt\n",
        "    if num_examples > 0:\n",
        "        example_imgs = [img for _, img in img_list[:num_examples]]\n",
        "        img_list = img_list[num_examples:]\n",
        "\n",
        "        prompt_func = create_few_shot_prompt\n",
        "    \n",
        "    results = []\n",
        "    for img_id, img in tqdm(img_list[:n]):\n",
        "        prompt = \"\"\n",
        "        if num_examples == 0:\n",
        "            prompt = prompt_func(img, k=k)\n",
        "        else:\n",
        "            prompt = prompt_func(img, example_imgs, k=k)\n",
        "        \n",
        "        gen_cap = \"\"\n",
        "        if model_name == \"gpt-3\":\n",
        "            this_message = messages + [{\"role\": \"user\", \"content\": prompt}]\n",
        "            response = openai.ChatCompletion.create(\n",
        "                model=\"gpt-3.5-turbo\",\n",
        "                messages=this_message,\n",
        "                max_tokens=max_new_tokens,\n",
        "                temperature=0.7\n",
        "            )\n",
        "            gen_cap = response['choices'][0]['message']['content']\n",
        "        else:\n",
        "            llm_input = tokenizer(prompt, return_tensors=\"pt\").to(device)\n",
        "            llm_output = model.generate(**llm_input, generation_config=generation_config)\n",
        "            decoded_llm_output = tokenizer.batch_decode(llm_output, skip_special_tokens=True)\n",
        "\n",
        "            gen_cap = decoded_output[0][len(prompt):].split(\"\\n\")[0]\n",
        "\n",
        "        results.append({\"image_id\": int(img_id), \"caption\": gen_cap})\n",
        "\n",
        "    return results        \n"
      ],
      "metadata": {
        "id": "jyFgQs3Dnces"
      },
      "execution_count": 26,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "results = prompting(outputs_captions, device, n=100, num_examples=0, k=5, warmup=warmup)\n",
        "\n",
        "res_file = f\"/content/gdrive/MyDrive/Semester 8/COS 485/Final Project/captions_val2017_{model_name}.json\"\n",
        "with open(res_file, \"w\") as outfile:\n",
        "    json.dump(results, outfile)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "sdZILXjofUtA",
        "outputId": "8960e2ee-502b-4311-beb0-b60973d87ce9"
      },
      "execution_count": 27,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 100/100 [02:46<00:00,  1.67s/it]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import locale\n",
        "def getpreferredencoding(do_setlocale = True):\n",
        "    return \"UTF-8\"\n",
        "locale.getpreferredencoding = getpreferredencoding"
      ],
      "metadata": {
        "id": "fjn49eQpG1wv"
      },
      "execution_count": 28,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Download captions\n",
        "!wget http://images.cocodataset.org/annotations/annotations_trainval2017.zip\n",
        "#Extract archive \n",
        "!unzip annotations_trainval2017.zip\n",
        "!rm annotations_trainval2017.zip"
      ],
      "metadata": {
        "id": "zg_bPApLGq-n",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "9c6e6b5e-3820-49bf-8809-e39037f4a4ae"
      },
      "execution_count": 29,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--2023-05-03 00:28:17--  http://images.cocodataset.org/annotations/annotations_trainval2017.zip\n",
            "Resolving images.cocodataset.org (images.cocodataset.org)... 54.231.194.137, 3.5.28.242, 54.231.233.33, ...\n",
            "Connecting to images.cocodataset.org (images.cocodataset.org)|54.231.194.137|:80... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 252907541 (241M) [application/zip]\n",
            "Saving to: ‘annotations_trainval2017.zip’\n",
            "\n",
            "annotations_trainva 100%[===================>] 241.19M  15.9MB/s    in 17s     \n",
            "\n",
            "2023-05-03 00:28:35 (14.3 MB/s) - ‘annotations_trainval2017.zip’ saved [252907541/252907541]\n",
            "\n",
            "Archive:  annotations_trainval2017.zip\n",
            "  inflating: annotations/instances_train2017.json  \n",
            "  inflating: annotations/instances_val2017.json  \n",
            "  inflating: annotations/captions_train2017.json  \n",
            "  inflating: annotations/captions_val2017.json  \n",
            "  inflating: annotations/person_keypoints_train2017.json  \n",
            "  inflating: annotations/person_keypoints_val2017.json  \n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "requested_model_name = model_name\n",
        "res_file = f\"/content/gdrive/MyDrive/Semester 8/COS 485/Final Project/captions_val2017_{requested_model_name}.json\"\n",
        "with open(res_file, \"r\") as f:\n",
        "    anns = json.load(f)\n",
        "annsImgIds = [ann['image_id'] for ann in anns]\n",
        "\n",
        "ann_file = './annotations/captions_val2017.json'\n",
        "            \n",
        "coco_caps=COCO(ann_file)\n",
        "cocoRes = coco_caps.loadRes(res_file)\n",
        "cocoEval = COCOEvalCap(coco_caps, cocoRes)\n",
        "cocoEval.params['image_id'] = cocoRes.getImgIds()\n",
        "cocoEval.evaluate()"
      ],
      "metadata": {
        "id": "e7yyj7WHGQcb",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "30e60cfe-00ce-4cf1-bede-80d2e96bcda9"
      },
      "execution_count": 30,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "loading annotations into memory...\n",
            "Done (t=0.04s)\n",
            "creating index...\n",
            "index created!\n",
            "Loading and preparing results...\n",
            "DONE (t=0.00s)\n",
            "creating index...\n",
            "index created!\n",
            "tokenization...\n",
            "setting up scorers...\n",
            "Downloading stanford-corenlp-3.6.0 for SPICE ...\n",
            "Progress: 384.5M / 384.5M (100.0%)\n",
            "Extracting stanford-corenlp-3.6.0 ...\n",
            "Done.\n",
            "computing Bleu score...\n",
            "{'testlen': 2004, 'reflen': 1273, 'guess': [2004, 1904, 1804, 1704], 'correct': [743, 213, 47, 14]}\n",
            "ratio: 1.574234092693186\n",
            "Bleu_1: 0.371\n",
            "Bleu_2: 0.204\n",
            "Bleu_3: 0.103\n",
            "Bleu_4: 0.055\n",
            "computing METEOR score...\n",
            "METEOR: 0.174\n",
            "computing Rouge score...\n",
            "ROUGE_L: 0.327\n",
            "computing CIDEr score...\n",
            "CIDEr: 0.195\n",
            "computing SPICE score...\n",
            "SPICE: 0.117\n"
          ]
        }
      ]
    }
  ]
}